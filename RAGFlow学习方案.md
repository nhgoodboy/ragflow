# RAGFlow æ¡†æ¶å­¦ä¹ æ–¹æ¡ˆ

## ğŸ“š å­¦ä¹ è·¯çº¿å›¾

```mermaid
graph TD
    A[åŸºç¡€å‡†å¤‡é˜¶æ®µ] --> B[ç†è®ºåŸºç¡€å­¦ä¹ ]
    B --> C[ç¯å¢ƒæ­å»ºä¸ä½“éªŒ]
    C --> D[æ ¸å¿ƒåŠŸèƒ½å­¦ä¹ ]
    D --> E[é¡¹ç›®å®è·µ]
    E --> F[è¿›é˜¶å¼€å‘]
    F --> G[æ·±åº¦å®šåˆ¶]
    
    A --> A1[PythonåŸºç¡€]
    A --> A2[DockeråŸºç¡€]
    A --> A3[Webå¼€å‘åŸºç¡€]
    
    B --> B1[RAGç†è®º]
    B --> B2[å‘é‡æ•°æ®åº“]
    B --> B3[LLMåŸºç¡€]
    
    C --> C1[Dockeréƒ¨ç½²]
    C --> C2[åŸºç¡€é…ç½®]
    C --> C3[ç•Œé¢ä½“éªŒ]
    
    D --> D1[æ–‡æ¡£å¤„ç†]
    D --> D2[çŸ¥è¯†åº“ç®¡ç†]
    D --> D3[é—®ç­”ç³»ç»Ÿ]
    D --> D4[AgentåŠŸèƒ½]
    
    E --> E1[ä¼ä¸šæ–‡æ¡£é—®ç­”]
    E --> E2[å¤šæ¨¡æ€RAG]
    E --> E3[APIé›†æˆ]
    
    F --> F1[è‡ªå®šä¹‰ç»„ä»¶]
    F --> F2[æ¨¡å‹é›†æˆ]
    F --> F3[æ€§èƒ½ä¼˜åŒ–]
    
    G --> G1[æ¶æ„æ‰©å±•]
    G --> G2[ä¼ä¸šçº§éƒ¨ç½²]
```

## ğŸ¯ å­¦ä¹ ç›®æ ‡è®¾å®š

### åˆçº§ç›®æ ‡ï¼ˆ1-2å‘¨ï¼‰
- âœ… ç†è§£RAGçš„åŸºæœ¬æ¦‚å¿µå’ŒåŸç†
- âœ… æˆåŠŸéƒ¨ç½²RAGFlowç³»ç»Ÿ
- âœ… å®ŒæˆåŸºç¡€çš„æ–‡æ¡£ä¸Šä¼ å’Œé—®ç­”æ“ä½œ
- âœ… ç†Ÿæ‚‰Webç•Œé¢çš„å„é¡¹åŠŸèƒ½

### ä¸­çº§ç›®æ ‡ï¼ˆ3-4å‘¨ï¼‰
- âœ… æŒæ¡çŸ¥è¯†åº“å’Œæœºå™¨äººçš„é…ç½®
- âœ… ç†è§£ä¸åŒè§£ææ¨¡å¼çš„é€‚ç”¨åœºæ™¯
- âœ… èƒ½å¤Ÿé€šè¿‡APIè¿›è¡Œç³»ç»Ÿé›†æˆ
- âœ… å®Œæˆä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§åº”ç”¨

### é«˜çº§ç›®æ ‡ï¼ˆ5-8å‘¨ï¼‰
- âœ… æŒæ¡Agentå·¥ä½œæµçš„è®¾è®¡
- âœ… èƒ½å¤Ÿå¼€å‘è‡ªå®šä¹‰ç»„ä»¶
- âœ… ç†è§£ç³»ç»Ÿæ¶æ„å¹¶è¿›è¡Œæ€§èƒ½ä¼˜åŒ–
- âœ… å…·å¤‡ä¼ä¸šçº§éƒ¨ç½²å’Œè¿ç»´èƒ½åŠ›

## ğŸ“– ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å‡†å¤‡ï¼ˆå»ºè®®æ—¶é—´ï¼š3-5å¤©ï¼‰

### 1.1 æŠ€æœ¯æ ˆåŸºç¡€

#### PythonåŸºç¡€
```python
# å¿…é¡»æŒæ¡çš„Pythonæ¦‚å¿µ
- é¢å‘å¯¹è±¡ç¼–ç¨‹
- å¼‚æ­¥ç¼–ç¨‹ï¼ˆasync/awaitï¼‰
- åŒ…ç®¡ç†ï¼ˆpip, uvï¼‰
- è™šæ‹Ÿç¯å¢ƒ
- å¸¸ç”¨åº“ï¼šrequests, json, pandas

# æ¨èå­¦ä¹ èµ„æº
- Pythonå®˜æ–¹æ•™ç¨‹
- ã€ŠPythonç¼–ç¨‹ï¼šä»å…¥é—¨åˆ°å®è·µã€‹
- åœ¨çº¿ç»ƒä¹ ï¼šLeetCode Pythoné¢˜ç›®
```

#### DockeråŸºç¡€
```bash
# å¿…é¡»æŒæ¡çš„Dockeræ¦‚å¿µ
docker run          # è¿è¡Œå®¹å™¨
docker compose      # å®¹å™¨ç¼–æ’
docker logs         # æŸ¥çœ‹æ—¥å¿—
docker ps           # æŸ¥çœ‹å®¹å™¨çŠ¶æ€
docker exec         # è¿›å…¥å®¹å™¨

# æ¨èå­¦ä¹ èµ„æº
- Dockerå®˜æ–¹æ–‡æ¡£
- ã€ŠDockeræŠ€æœ¯å…¥é—¨ä¸å®æˆ˜ã€‹
```

#### Webå¼€å‘åŸºç¡€
```javascript
# å‰ç«¯åŸºç¡€ï¼ˆå¯é€‰ï¼Œä¸»è¦ç”¨äºç†è§£ç•Œé¢ï¼‰
- HTML/CSSåŸºç¡€
- JavaScript ES6+
- ReactåŸºç¡€æ¦‚å¿µ
- RESTful APIæ¦‚å¿µ

# æ¨èå­¦ä¹ èµ„æº
- MDN Webæ–‡æ¡£
- Reactå®˜æ–¹æ•™ç¨‹
```

### 1.2 ç¯å¢ƒå‡†å¤‡æ¸…å•

```bash
# ç¡¬ä»¶è¦æ±‚æ£€æŸ¥
âœ“ CPU: 4æ ¸å¿ƒä»¥ä¸Š
âœ“ å†…å­˜: 16GBä»¥ä¸Š
âœ“ ç¡¬ç›˜: 50GBå¯ç”¨ç©ºé—´
âœ“ ç½‘ç»œ: ç¨³å®šçš„äº’è”ç½‘è¿æ¥

# è½¯ä»¶å®‰è£…æ¸…å•
âœ“ Docker >= 24.0.0
âœ“ Docker Compose >= v2.26.1
âœ“ Git
âœ“ ä»£ç ç¼–è¾‘å™¨ï¼ˆVSCodeæ¨èï¼‰
âœ“ æµè§ˆå™¨ï¼ˆChrome/Firefoxï¼‰

# å¯é€‰å·¥å…·
âœ“ Postmanï¼ˆAPIæµ‹è¯•ï¼‰
âœ“ MySQLå®¢æˆ·ç«¯
âœ“ Rediså®¢æˆ·ç«¯
```

## ğŸ§  ç¬¬äºŒé˜¶æ®µï¼šç†è®ºåŸºç¡€å­¦ä¹ ï¼ˆå»ºè®®æ—¶é—´ï¼š5-7å¤©ï¼‰

### 2.1 RAGæ ¸å¿ƒæ¦‚å¿µ

#### ä»€ä¹ˆæ˜¯RAGï¼Ÿ
```
RAG = Retrieval-Augmented Generation
æ£€ç´¢å¢å¼ºç”Ÿæˆ = ä¿¡æ¯æ£€ç´¢ + æ–‡æœ¬ç”Ÿæˆ

æ ¸å¿ƒæµç¨‹ï¼š
1. æ–‡æ¡£é¢„å¤„ç†å’Œå‘é‡åŒ–
2. ç”¨æˆ·æŸ¥è¯¢çš„å‘é‡åŒ–
3. ç›¸ä¼¼æ€§æœç´¢æ‰¾åˆ°ç›¸å…³æ–‡æ¡£
4. å°†æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡ä¼ é€’ç»™LLM
5. ç”Ÿæˆå¢å¼ºçš„å›ç­”
```

#### RAG vs å¾®è°ƒæ¨¡å‹
```
RAGä¼˜åŠ¿ï¼š
âœ“ çŸ¥è¯†å¯ä»¥å®æ—¶æ›´æ–°
âœ“ æˆæœ¬æ›´ä½
âœ“ å¯è§£é‡Šæ€§å¼ºï¼ˆå¯è¿½æº¯æ¥æºï¼‰
âœ“ é€‚åˆä¼ä¸šçŸ¥è¯†åº“

å¾®è°ƒä¼˜åŠ¿ï¼š
âœ“ å“åº”é€Ÿåº¦å¿«
âœ“ çŸ¥è¯†èå…¥æ¨¡å‹å‚æ•°
âœ“ é€‚åˆç‰¹å®šé¢†åŸŸæ·±åº¦å®šåˆ¶
```

### 2.2 å‘é‡æ•°æ®åº“åŸç†

#### å‘é‡åŒ–è¿‡ç¨‹
```python
# æ–‡æœ¬å‘é‡åŒ–ç¤ºä¾‹
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
text = "è¿™æ˜¯ä¸€æ®µç¤ºä¾‹æ–‡æ¡£"
embedding = model.encode(text)
print(f"å‘é‡ç»´åº¦: {len(embedding)}")  # é€šå¸¸æ˜¯384æˆ–768ç»´
```

#### ç›¸ä¼¼æ€§æœç´¢
```python
# ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
import numpy as np

def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

similarity = cosine_similarity(query_vector, document_vector)
```

### 2.3 å¤§è¯­è¨€æ¨¡å‹åŸºç¡€

#### æç¤ºè¯å·¥ç¨‹
```
ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿ï¼š
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š
{question}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæ–‡æ¡£å†…å®¹ï¼Œå‡†ç¡®å›ç­”é—®é¢˜
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. æä¾›æ–‡æ¡£æ¥æºå¼•ç”¨
```

## ğŸ› ï¸ ç¬¬ä¸‰é˜¶æ®µï¼šç¯å¢ƒæ­å»ºä¸ä½“éªŒï¼ˆå»ºè®®æ—¶é—´ï¼š2-3å¤©ï¼‰

### 3.1 å¿«é€Ÿéƒ¨ç½²

#### Step 1: å…‹éš†é¡¹ç›®
```bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow
```

#### Step 2: ç¯å¢ƒé…ç½®
```bash
# ç³»ç»Ÿå‚æ•°è®¾ç½®
sudo sysctl -w vm.max_map_count=262144

# å¤åˆ¶ç¯å¢ƒé…ç½®
cp docker/.env.example docker/.env

# ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
vim docker/.env
```

#### Step 3: å¯åŠ¨æœåŠ¡
```bash
cd docker

# ä½¿ç”¨Infinityå‘é‡å¼•æ“ï¼ˆæ¨èï¼‰
docker compose --profile infinity up -d

# éªŒè¯éƒ¨ç½²
docker compose ps
curl http://localhost/health
```

### 3.2 åˆæ¬¡ä½“éªŒä»»åŠ¡

#### ä»»åŠ¡1: ç³»ç»Ÿç™»å½•
```
1. è®¿é—® http://localhost
2. ä½¿ç”¨é»˜è®¤è´¦æˆ·ç™»å½•: admin / infiniflow
3. ç†Ÿæ‚‰ä¸»ç•Œé¢å¸ƒå±€
```

#### ä»»åŠ¡2: åˆ›å»ºçŸ¥è¯†åº“
```
1. ç‚¹å‡»"çŸ¥è¯†åº“"èœå•
2. æ–°å»ºçŸ¥è¯†åº“ï¼š"æµ‹è¯•çŸ¥è¯†åº“"
3. é€‰æ‹©è§£ææ¨¡å¼ï¼š"æ™ºèƒ½æ¨¡å¼"
4. ä¸Šä¼ ä¸€ä¸ªPDFæ–‡ä»¶
5. è§‚å¯Ÿè§£æè¿‡ç¨‹å’Œç»“æœ
```

#### ä»»åŠ¡3: é…ç½®æœºå™¨äºº
```
1. ç‚¹å‡»"å¯¹è¯"èœå•
2. æ–°å»ºåŠ©æ‰‹ï¼š"æµ‹è¯•åŠ©æ‰‹"
3. å…³è”ä¹‹å‰åˆ›å»ºçš„çŸ¥è¯†åº“
4. è¿›è¡Œç®€å•çš„é—®ç­”æµ‹è¯•
```

### 3.3 å¸¸è§é—®é¢˜æ’æŸ¥

```bash
# å®¹å™¨çŠ¶æ€æ£€æŸ¥
docker compose ps

# æŸ¥çœ‹æ—¥å¿—
docker compose logs ragflow-server

# é‡å¯æœåŠ¡
docker compose restart ragflow-server

# æ¸…ç†å¹¶é‡æ–°éƒ¨ç½²
docker compose down
docker compose --profile infinity up -d
```

## ğŸ“Š ç¬¬å››é˜¶æ®µï¼šæ ¸å¿ƒåŠŸèƒ½å­¦ä¹ ï¼ˆå»ºè®®æ—¶é—´ï¼š7-10å¤©ï¼‰

### 4.1 æ–‡æ¡£å¤„ç†æ·±å…¥å­¦ä¹ 

#### æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
```
âœ“ PDFæ–‡æ¡£
âœ“ Wordæ–‡æ¡£ï¼ˆ.docxï¼‰
âœ“ PowerPointï¼ˆ.pptxï¼‰
âœ“ Excelæ–‡ä»¶ï¼ˆ.xlsxï¼‰
âœ“ æ–‡æœ¬æ–‡ä»¶ï¼ˆ.txtï¼‰
âœ“ Markdownæ–‡ä»¶ï¼ˆ.mdï¼‰
âœ“ å›¾ç‰‡æ–‡ä»¶ï¼ˆOCRï¼‰
```

#### è§£ææ¨¡å¼å¯¹æ¯”
```json
{
  "æ™ºèƒ½æ¨¡å¼": {
    "ç‰¹ç‚¹": "è‡ªåŠ¨è¯†åˆ«æ–‡æ¡£ç»“æ„",
    "é€‚ç”¨": "å¤æ‚å¸ƒå±€çš„æ–‡æ¡£",
    "å¤„ç†": "è¡¨æ ¼ã€å›¾ç‰‡ã€å¤šåˆ—å¸ƒå±€"
  },
  "ç®€å•æ¨¡å¼": {
    "ç‰¹ç‚¹": "åŸºç¡€æ–‡æœ¬æå–",
    "é€‚ç”¨": "çº¯æ–‡æœ¬æ–‡æ¡£",
    "å¤„ç†": "å¿«é€Ÿç®€å•å¤„ç†"
  },
  "æ‰‹åŠ¨æ¨¡å¼": {
    "ç‰¹ç‚¹": "è‡ªå®šä¹‰è§£æè§„åˆ™",
    "é€‚ç”¨": "ç‰¹æ®Šæ ¼å¼æ–‡æ¡£",
    "å¤„ç†": "å®Œå…¨å¯æ§çš„è§£ææµç¨‹"
  }
}
```

#### åˆ†å—ç­–ç•¥ä¼˜åŒ–
```python
# åˆ†å—å‚æ•°è°ƒä¼˜
chunk_config = {
    "chunk_size": 1000,        # å»ºè®®800-1500
    "chunk_overlap": 200,      # å»ºè®®chunk_sizeçš„10-20%
    "enable_ocr": True,        # å›¾ç‰‡æ–‡å­—è¯†åˆ«
    "enable_table": True,      # è¡¨æ ¼æå–
    "language": "zh-CN"        # è¯­è¨€è®¾ç½®
}

# ä¸åŒæ–‡æ¡£ç±»å‹çš„å»ºè®®é…ç½®
pdf_config = {"chunk_size": 1200, "chunk_overlap": 240}
txt_config = {"chunk_size": 800, "chunk_overlap": 160}
```

### 4.2 çŸ¥è¯†åº“ç®¡ç†å®è·µ

#### å®è·µé¡¹ç›®ï¼šæ„å»ºä¼ä¸šçŸ¥è¯†åº“
```
é¡¹ç›®ç›®æ ‡ï¼šä¸ºä¸€å®¶è½¯ä»¶å…¬å¸æ„å»ºæŠ€æœ¯æ–‡æ¡£çŸ¥è¯†åº“

æ–‡æ¡£ç±»å‹ï¼š
- äº§å“æ‰‹å†Œï¼ˆPDFï¼‰
- APIæ–‡æ¡£ï¼ˆMarkdownï¼‰
- ç”¨æˆ·æŒ‡å—ï¼ˆWordï¼‰
- å¸¸è§é—®é¢˜ï¼ˆExcelï¼‰

å®æ–½æ­¥éª¤ï¼š
1. æ–‡æ¡£åˆ†ç±»å’Œé¢„å¤„ç†
2. åˆ›å»ºåˆ†ç±»çŸ¥è¯†åº“
3. æ‰¹é‡ä¸Šä¼ å’Œè§£æ
4. æµ‹è¯•æ£€ç´¢æ•ˆæœ
5. ä¼˜åŒ–å‚æ•°é…ç½®
```

#### çŸ¥è¯†åº“ä¼˜åŒ–ç­–ç•¥
```python
# æ£€ç´¢æ•ˆæœè¯„ä¼°
def evaluate_retrieval(questions, expected_docs):
    results = []
    for question in questions:
        retrieved_docs = search_knowledge_base(question)
        precision = calculate_precision(retrieved_docs, expected_docs[question])
        recall = calculate_recall(retrieved_docs, expected_docs[question])
        results.append({"question": question, "precision": precision, "recall": recall})
    return results

# å‚æ•°è°ƒä¼˜å»ºè®®
optimization_tips = {
    "ä½å¬å›ç‡": "é™ä½similarity_thresholdï¼Œå¢åŠ top_k",
    "ä½ç²¾ç¡®ç‡": "æé«˜similarity_thresholdï¼Œä¼˜åŒ–æ–‡æ¡£è´¨é‡",
    "å“åº”æ…¢": "å‡å°‘chunk_sizeï¼Œä¼˜åŒ–ç¡¬ä»¶é…ç½®"
}
```

### 4.3 é—®ç­”ç³»ç»Ÿé…ç½®

#### æç¤ºè¯å·¥ç¨‹å®è·µ
```python
# åŸºç¡€æ¨¡æ¿
basic_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š
{question}

è¯·ä¸¥æ ¼åŸºäºæ–‡æ¡£å†…å®¹å›ç­”ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
"""

# ç»“æ„åŒ–å›ç­”æ¨¡æ¿
structured_template = """
è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç»„ç»‡ç­”æ¡ˆï¼š

## é—®é¢˜åˆ†æ
[å¯¹é—®é¢˜çš„ç†è§£å’Œåˆ†æ]

## æ ¸å¿ƒç­”æ¡ˆ
[åŸºäºæ–‡æ¡£çš„ç›´æ¥å›ç­”]

## è¯¦ç»†è¯´æ˜
[è¡¥å……ä¿¡æ¯å’Œè§£é‡Š]

## å‚è€ƒæ¥æº
[æ–‡æ¡£æ¥æºå’Œé¡µç ]

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š
{question}
"""
```

#### LLMå‚æ•°è°ƒä¼˜
```python
llm_config = {
    "temperature": 0.1,      # åˆ›æ„æ€§ vs å‡†ç¡®æ€§
    "max_tokens": 2000,      # å›ç­”é•¿åº¦é™åˆ¶
    "top_p": 0.9,           # æ ¸é‡‡æ ·å‚æ•°
    "frequency_penalty": 0,  # é‡å¤æƒ©ç½š
    "presence_penalty": 0    # ä¸»é¢˜æƒ©ç½š
}

# ä¸åŒåœºæ™¯çš„æ¨èé…ç½®
scenarios = {
    "äº‹å®é—®ç­”": {"temperature": 0.1, "top_p": 0.8},
    "åˆ›æ„å†™ä½œ": {"temperature": 0.7, "top_p": 0.9},
    "ä»£ç ç”Ÿæˆ": {"temperature": 0.0, "top_p": 0.8}
}
```

### 4.4 AgentåŠŸèƒ½å­¦ä¹ 

#### Agentå·¥ä½œæµè®¾è®¡
```python
# ç®€å•çš„Agentç»„ä»¶ç¤ºä¾‹
class DocumentAnalysisAgent:
    def __init__(self):
        self.components = [
            "æ–‡æ¡£ä¸Šä¼ ç»„ä»¶",
            "å†…å®¹æå–ç»„ä»¶", 
            "åˆ†æå¤„ç†ç»„ä»¶",
            "ç»“æœè¾“å‡ºç»„ä»¶"
        ]
    
    async def execute(self, document):
        # 1. æ–‡æ¡£é¢„å¤„ç†
        processed_doc = await self.preprocess(document)
        
        # 2. å†…å®¹åˆ†æ
        analysis = await self.analyze(processed_doc)
        
        # 3. ç”ŸæˆæŠ¥å‘Š
        report = await self.generate_report(analysis)
        
        return report
```

#### å®è·µé¡¹ç›®ï¼šæ„å»ºæ™ºèƒ½å®¢æœAgent
```yaml
é¡¹ç›®éœ€æ±‚ï¼š
  - è‡ªåŠ¨åˆ†ç±»ç”¨æˆ·é—®é¢˜
  - æ£€ç´¢ç›¸å…³è§£å†³æ–¹æ¡ˆ
  - ç”Ÿæˆä¸ªæ€§åŒ–å›ç­”
  - escalateå¤æ‚é—®é¢˜

æŠ€æœ¯å®ç°ï¼š
  - é—®é¢˜åˆ†ç±»ç»„ä»¶
  - çŸ¥è¯†åº“æ£€ç´¢ç»„ä»¶
  - å›ç­”ç”Ÿæˆç»„ä»¶
  - äººå·¥è½¬æ¥ç»„ä»¶
```

## ğŸš€ ç¬¬äº”é˜¶æ®µï¼šé¡¹ç›®å®è·µï¼ˆå»ºè®®æ—¶é—´ï¼š10-14å¤©ï¼‰

### 5.1 é¡¹ç›®1ï¼šä¼ä¸šæ–‡æ¡£é—®ç­”ç³»ç»Ÿ

#### é¡¹ç›®èƒŒæ™¯
```
æŸåˆ¶é€ ä¼ä¸šéœ€è¦æ„å»ºå†…éƒ¨æŠ€æœ¯æ–‡æ¡£é—®ç­”ç³»ç»Ÿï¼ŒåŒ…å«ï¼š
- è®¾å¤‡æ“ä½œæ‰‹å†Œ
- å®‰å…¨è§„ç¨‹æ–‡æ¡£
- ç»´ä¿®æŒ‡å¯¼ä¹¦
- è´¨é‡æ ‡å‡†æ–‡æ¡£
```

#### å®æ–½è®¡åˆ’
```
Week 1: éœ€æ±‚åˆ†æå’Œæ¶æ„è®¾è®¡
- æ”¶é›†å’Œåˆ†ææ–‡æ¡£ç±»å‹
- è®¾è®¡çŸ¥è¯†åº“ç»“æ„
- ç¡®å®šç”¨æˆ·è§’è‰²å’Œæƒé™

Week 2: ç³»ç»Ÿéƒ¨ç½²å’Œé…ç½®
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- çŸ¥è¯†åº“åˆ›å»ºå’Œæ–‡æ¡£å¯¼å…¥
- æœºå™¨äººé…ç½®å’Œè°ƒä¼˜

Week 3: æµ‹è¯•å’Œä¼˜åŒ–
- åŠŸèƒ½æµ‹è¯•å’Œæ€§èƒ½æµ‹è¯•
- ç”¨æˆ·æµ‹è¯•å’Œåé¦ˆæ”¶é›†
- ç³»ç»Ÿä¼˜åŒ–å’Œè¿­ä»£
```

#### å…³é”®æŠ€æœ¯ç‚¹
```python
# å¤šçŸ¥è¯†åº“æ£€ç´¢ç­–ç•¥
def multi_kb_search(query, knowledge_bases):
    results = []
    for kb in knowledge_bases:
        kb_results = kb.search(query, top_k=3)
        results.extend(kb_results)
    
    # é‡æ–°æ’åºå’Œå»é‡
    ranked_results = rerank_results(results, query)
    return ranked_results[:10]

# æƒé™æ§åˆ¶å®ç°
def check_access_permission(user, knowledge_base):
    user_roles = get_user_roles(user)
    required_roles = knowledge_base.get_required_roles()
    return any(role in user_roles for role in required_roles)
```

### 5.2 é¡¹ç›®2ï¼šå¤šæ¨¡æ€RAGç³»ç»Ÿ

#### é¡¹ç›®ç›®æ ‡
```
æ„å»ºæ”¯æŒå›¾æ–‡æ··åˆå†…å®¹çš„RAGç³»ç»Ÿï¼š
- å¤„ç†åŒ…å«å›¾è¡¨çš„æŠ€æœ¯æ–‡æ¡£
- æ”¯æŒå›¾ç‰‡å†…å®¹æ£€ç´¢
- ç”Ÿæˆå›¾æ–‡å¹¶èŒ‚çš„å›ç­”
```

#### æŠ€æœ¯æ¶æ„
```python
class MultiModalRAGSystem:
    def __init__(self):
        self.text_processor = TextProcessor()
        self.image_processor = ImageProcessor()
        self.multimodal_embedder = MultiModalEmbedder()
    
    async def process_document(self, doc_path):
        # æå–æ–‡æœ¬å’Œå›¾ç‰‡
        text_content = self.text_processor.extract(doc_path)
        images = self.image_processor.extract_images(doc_path)
        
        # å¤šæ¨¡æ€å‘é‡åŒ–
        embeddings = []
        for content in text_content:
            embedding = self.multimodal_embedder.encode_text(content)
            embeddings.append(embedding)
        
        for image in images:
            embedding = self.multimodal_embedder.encode_image(image)
            embeddings.append(embedding)
        
        return embeddings
```

### 5.3 é¡¹ç›®3ï¼šAPIé›†æˆä¸è‡ªå®šä¹‰å¼€å‘

#### RESTful APIé›†æˆ
```python
import requests
import asyncio

class RAGFlowClient:
    def __init__(self, base_url, api_key):
        self.base_url = base_url
        self.headers = {"Authorization": f"Bearer {api_key}"}
    
    async def upload_document(self, file_path, kb_id):
        url = f"{self.base_url}/api/v1/document/upload"
        
        with open(file_path, 'rb') as f:
            files = {'file': f}
            data = {'kb_id': kb_id}
            
            response = requests.post(url, files=files, data=data, headers=self.headers)
            return response.json()
    
    async def ask_question(self, question, kb_id):
        url = f"{self.base_url}/api/v1/chat/completion"
        
        payload = {
            'question': question,
            'kb_id': kb_id,
            'stream': False
        }
        
        response = requests.post(url, json=payload, headers=self.headers)
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    client = RAGFlowClient("http://localhost", "your-api-key")
    
    # ä¸Šä¼ æ–‡æ¡£
    result = await client.upload_document("document.pdf", "kb_123")
    print(f"ä¸Šä¼ ç»“æœ: {result}")
    
    # æé—®
    answer = await client.ask_question("è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ", "kb_123")
    print(f"å›ç­”: {answer}")

asyncio.run(main())
```

## ğŸ”§ ç¬¬å…­é˜¶æ®µï¼šè¿›é˜¶å¼€å‘ï¼ˆå»ºè®®æ—¶é—´ï¼š14-21å¤©ï¼‰

### 6.1 è‡ªå®šä¹‰ç»„ä»¶å¼€å‘

#### å¼€å‘è‡ªå®šä¹‰æ–‡æ¡£è§£æå™¨
```python
# deepdoc/parser/excel_parser.py
import pandas as pd
from deepdoc.parser.base import BaseParser

class ExcelParser(BaseParser):
    def __init__(self):
        super().__init__()
        self.supported_formats = ['.xlsx', '.xls']
    
    def parse(self, file_path, **kwargs):
        """è§£æExcelæ–‡ä»¶ä¸ºç»“æ„åŒ–æ•°æ®"""
        try:
            # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
            excel_file = pd.ExcelFile(file_path)
            chunks = []
            
            for sheet_name in excel_file.sheet_names:
                df = pd.read_excel(file_path, sheet_name=sheet_name)
                
                # è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
                sheet_text = f"å·¥ä½œè¡¨: {sheet_name}\n"
                sheet_text += df.to_string(index=False)
                
                chunks.append({
                    'content': sheet_text,
                    'metadata': {
                        'sheet_name': sheet_name,
                        'rows': len(df),
                        'columns': len(df.columns)
                    }
                })
            
            return chunks
            
        except Exception as e:
            raise Exception(f"Excelè§£æå¤±è´¥: {str(e)}")

# æ³¨å†Œè§£æå™¨
from deepdoc.parser.parser_factory import ParserFactory
ParserFactory.register('excel', ExcelParser)
```

#### å¼€å‘è‡ªå®šä¹‰æ£€ç´¢å™¨
```python
# rag/retrieval/hybrid_retriever.py
from rag.retrieval.base import BaseRetriever

class HybridRetriever(BaseRetriever):
    """æ··åˆæ£€ç´¢å™¨ï¼šç»“åˆå…³é”®è¯å’Œè¯­ä¹‰æ£€ç´¢"""
    
    def __init__(self, config):
        super().__init__(config)
        self.keyword_weight = config.get('keyword_weight', 0.3)
        self.semantic_weight = config.get('semantic_weight', 0.7)
    
    async def retrieve(self, query, top_k=10):
        # å…³é”®è¯æ£€ç´¢
        keyword_results = await self._keyword_search(query, top_k * 2)
        
        # è¯­ä¹‰æ£€ç´¢
        semantic_results = await self._semantic_search(query, top_k * 2)
        
        # ç»“æœèåˆ
        hybrid_results = self._merge_results(
            keyword_results, semantic_results, 
            self.keyword_weight, self.semantic_weight
        )
        
        return hybrid_results[:top_k]
    
    def _merge_results(self, keyword_results, semantic_results, kw_weight, sem_weight):
        """èåˆæ£€ç´¢ç»“æœ"""
        merged = {}
        
        # åŠ æƒåˆå¹¶åˆ†æ•°
        for result in keyword_results:
            doc_id = result['doc_id']
            merged[doc_id] = {
                'content': result['content'],
                'score': result['score'] * kw_weight,
                'metadata': result['metadata']
            }
        
        for result in semantic_results:
            doc_id = result['doc_id']
            if doc_id in merged:
                merged[doc_id]['score'] += result['score'] * sem_weight
            else:
                merged[doc_id] = {
                    'content': result['content'],
                    'score': result['score'] * sem_weight,
                    'metadata': result['metadata']
                }
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_results = sorted(merged.values(), key=lambda x: x['score'], reverse=True)
        return sorted_results
```

### 6.2 æ€§èƒ½ä¼˜åŒ–å®è·µ

#### ç¼“å­˜ç­–ç•¥å®ç°
```python
import redis
import json
import hashlib

class RAGCache:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
        self.ttl = 3600  # ç¼“å­˜1å°æ—¶
    
    def _generate_key(self, query, kb_id):
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{query}:{kb_id}"
        return hashlib.md5(content.encode()).hexdigest()
    
    async def get_cached_result(self, query, kb_id):
        """è·å–ç¼“å­˜ç»“æœ"""
        key = self._generate_key(query, kb_id)
        cached = self.redis_client.get(key)
        
        if cached:
            return json.loads(cached)
        return None
    
    async def cache_result(self, query, kb_id, result):
        """ç¼“å­˜ç»“æœ"""
        key = self._generate_key(query, kb_id)
        self.redis_client.setex(key, self.ttl, json.dumps(result))

# é›†æˆåˆ°RAGç³»ç»Ÿ
class CachedRAGEngine:
    def __init__(self):
        self.cache = RAGCache()
        self.rag_engine = RAGEngine()
    
    async def answer_question(self, query, kb_id):
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = await self.cache.get_cached_result(query, kb_id)
        if cached_result:
            return cached_result
        
        # ç”Ÿæˆæ–°å›ç­”
        result = await self.rag_engine.answer_question(query, kb_id)
        
        # ç¼“å­˜ç»“æœ
        await self.cache.cache_result(query, kb_id, result)
        
        return result
```

#### å¹¶å‘å¤„ç†ä¼˜åŒ–
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class ParallelDocumentProcessor:
    def __init__(self, max_workers=4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def process_documents_batch(self, documents):
        """æ‰¹é‡å¹¶è¡Œå¤„ç†æ–‡æ¡£"""
        loop = asyncio.get_event_loop()
        tasks = []
        
        for doc in documents:
            task = loop.run_in_executor(
                self.executor, 
                self._process_single_document, 
                doc
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        successful_results = [
            result for result in results 
            if not isinstance(result, Exception)
        ]
        
        return successful_results
    
    def _process_single_document(self, document):
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        try:
            # æ–‡æ¡£è§£æé€»è¾‘
            chunks = self.parser.parse(document)
            # å‘é‡åŒ–
            embeddings = self.embedder.encode(chunks)
            # å­˜å‚¨
            self.vector_store.add(embeddings)
            return {"status": "success", "doc_id": document.id}
        except Exception as e:
            return {"status": "error", "error": str(e)}
```

### 6.3 ç›‘æ§ä¸è¿ç»´

#### ç³»ç»Ÿç›‘æ§å®ç°
```python
import time
import psutil
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# å®šä¹‰ç›‘æ§æŒ‡æ ‡
REQUEST_COUNT = Counter('ragflow_requests_total', 'Total requests', ['method', 'endpoint'])
REQUEST_LATENCY = Histogram('ragflow_request_duration_seconds', 'Request latency')
ACTIVE_CONNECTIONS = Gauge('ragflow_active_connections', 'Active connections')
MEMORY_USAGE = Gauge('ragflow_memory_usage_bytes', 'Memory usage')

class RAGFlowMonitor:
    def __init__(self):
        self.start_time = time.time()
    
    def record_request(self, method, endpoint, duration):
        """è®°å½•è¯·æ±‚æŒ‡æ ‡"""
        REQUEST_COUNT.labels(method=method, endpoint=endpoint).inc()
        REQUEST_LATENCY.observe(duration)
    
    def update_system_metrics(self):
        """æ›´æ–°ç³»ç»ŸæŒ‡æ ‡"""
        # å†…å­˜ä½¿ç”¨ç‡
        memory = psutil.virtual_memory()
        MEMORY_USAGE.set(memory.used)
        
        # æ´»è·ƒè¿æ¥æ•°ï¼ˆç¤ºä¾‹ï¼‰
        # ACTIVE_CONNECTIONS.set(get_active_connections())
    
    def start_monitoring(self, port=8000):
        """å¯åŠ¨ç›‘æ§æœåŠ¡"""
        start_http_server(port)
        print(f"ç›‘æ§æœåŠ¡å¯åŠ¨åœ¨ç«¯å£ {port}")

# ä½¿ç”¨è£…é¥°å™¨è®°å½•æ€§èƒ½
def monitor_performance(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            monitor.record_request('POST', func.__name__, duration)
            return result
        except Exception as e:
            duration = time.time() - start_time
            monitor.record_request('POST', func.__name__, duration)
            raise e
    return wrapper

# åº”ç”¨åˆ°RAGå‡½æ•°
monitor = RAGFlowMonitor()

@monitor_performance
def answer_question(query, kb_id):
    # RAGé€»è¾‘
    pass
```

## ğŸ“ˆ ç¬¬ä¸ƒé˜¶æ®µï¼šæ·±åº¦å®šåˆ¶ï¼ˆå»ºè®®æ—¶é—´ï¼š21-30å¤©ï¼‰

### 7.1 æ¶æ„æ‰©å±•

#### å¾®æœåŠ¡æ‹†åˆ†è®¾è®¡
```yaml
# docker-compose-microservices.yml
version: '3.8'
services:
  # æ–‡æ¡£å¤„ç†æœåŠ¡
  document-processor:
    image: ragflow/document-processor:latest
    ports:
      - "8001:8000"
    environment:
      - SERVICE_NAME=document-processor
  
  # æ£€ç´¢æœåŠ¡
  retrieval-service:
    image: ragflow/retrieval-service:latest
    ports:
      - "8002:8000"
    environment:
      - SERVICE_NAME=retrieval-service
  
  # ç”ŸæˆæœåŠ¡
  generation-service:
    image: ragflow/generation-service:latest
    ports:
      - "8003:8000"
    environment:
      - SERVICE_NAME=generation-service
  
  # APIç½‘å…³
  api-gateway:
    image: ragflow/api-gateway:latest
    ports:
      - "8000:8000"
    depends_on:
      - document-processor
      - retrieval-service
      - generation-service
```

#### åˆ†å¸ƒå¼éƒ¨ç½²é…ç½®
```python
# config/distributed_config.py
class DistributedConfig:
    def __init__(self):
        self.services = {
            'document_processor': {
                'instances': 3,
                'resources': {'cpu': '2', 'memory': '4Gi'},
                'scaling': {'min': 2, 'max': 10}
            },
            'retrieval_service': {
                'instances': 5,
                'resources': {'cpu': '1', 'memory': '2Gi'},
                'scaling': {'min': 3, 'max': 20}
            },
            'generation_service': {
                'instances': 2,
                'resources': {'cpu': '4', 'memory': '8Gi'},
                'scaling': {'min': 1, 'max': 5}
            }
        }
```

### 7.2 ä¼ä¸šçº§éƒ¨ç½²

#### Kuberneteséƒ¨ç½²é…ç½®
```yaml
# k8s/ragflow-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ragflow-server
  namespace: ragflow
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ragflow-server
  template:
    metadata:
      labels:
        app: ragflow-server
    spec:
      containers:
      - name: ragflow-server
        image: infiniflow/ragflow:v0.19.0
        ports:
        - containerPort: 9380
        env:
        - name: MYSQL_HOST
          value: "mysql-service"
        - name: REDIS_HOST
          value: "redis-service"
        resources:
          limits:
            cpu: "2"
            memory: "4Gi"
          requests:
            cpu: "1"
            memory: "2Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 9380
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 9380
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: ragflow-service
  namespace: ragflow
spec:
  selector:
    app: ragflow-server
  ports:
  - port: 80
    targetPort: 9380
  type: LoadBalancer
```

#### é«˜å¯ç”¨é…ç½®
```yaml
# æ•°æ®åº“ä¸»ä»é…ç½®
mysql-master:
  image: mysql:8.0
  environment:
    - MYSQL_REPLICATION_MODE=master
    - MYSQL_REPLICATION_USER=replicator
    - MYSQL_REPLICATION_PASSWORD=password

mysql-slave:
  image: mysql:8.0
  environment:
    - MYSQL_REPLICATION_MODE=slave
    - MYSQL_MASTER_HOST=mysql-master
    - MYSQL_REPLICATION_USER=replicator
    - MYSQL_REPLICATION_PASSWORD=password

# Redisé›†ç¾¤é…ç½®
redis-cluster:
  image: redis:7
  command: redis-server --cluster-enabled yes --cluster-config-file nodes.conf
  deploy:
    replicas: 6
```

## ğŸ“ å­¦ä¹ æˆæœæ£€éªŒ

### é˜¶æ®µæ€§æµ‹è¯•é¢˜

#### åŸºç¡€æµ‹è¯•ï¼ˆç¬¬1-3é˜¶æ®µï¼‰
```
1. è§£é‡ŠRAGçš„å·¥ä½œåŸç†å’Œä¼˜åŠ¿
2. æè¿°å‘é‡æ•°æ®åº“çš„ä½œç”¨
3. æˆåŠŸéƒ¨ç½²RAGFlowå¹¶åˆ›å»ºçŸ¥è¯†åº“
4. é…ç½®ä¸€ä¸ªç®€å•çš„é—®ç­”æœºå™¨äºº
```

#### è¿›é˜¶æµ‹è¯•ï¼ˆç¬¬4-5é˜¶æ®µï¼‰
```
1. è®¾è®¡å¹¶å®ç°ä¸€ä¸ªä¼ä¸šçº§çŸ¥è¯†åº“ç³»ç»Ÿ
2. ä¼˜åŒ–æ–‡æ¡£åˆ†å—å’Œæ£€ç´¢å‚æ•°
3. å¼€å‘APIé›†æˆåº”ç”¨
4. æ„å»ºå¤šæ¨¡æ€RAGç³»ç»Ÿ
```

#### é«˜çº§æµ‹è¯•ï¼ˆç¬¬6-7é˜¶æ®µï¼‰
```
1. å¼€å‘è‡ªå®šä¹‰ç»„ä»¶å¹¶é›†æˆåˆ°ç³»ç»Ÿ
2. å®ç°ç³»ç»Ÿæ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–
3. è®¾è®¡åˆ†å¸ƒå¼æ¶æ„æ–¹æ¡ˆ
4. å®Œæˆä¼ä¸šçº§éƒ¨ç½²å’Œè¿ç»´
```

### å®æˆ˜é¡¹ç›®å»ºè®®

#### åˆçº§é¡¹ç›®
- ä¸ªäººæ–‡æ¡£ç®¡ç†ç³»ç»Ÿ
- ç®€å•çš„FAQæœºå™¨äºº
- æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå™¨

#### ä¸­çº§é¡¹ç›®
- ä¼ä¸šçŸ¥è¯†åº“ç³»ç»Ÿ
- å¤šè¯­è¨€æ–‡æ¡£é—®ç­”
- APIæœåŠ¡é›†æˆ

#### é«˜çº§é¡¹ç›®
- åˆ†å¸ƒå¼RAGå¹³å°
- å¤šæ¨¡æ€å†…å®¹ç†è§£
- ä¼ä¸šçº§æ™ºèƒ½åŠ©æ‰‹

## ğŸ“š æ¨èå­¦ä¹ èµ„æº

### å®˜æ–¹èµ„æº
- [RAGFlow GitHub](https://github.com/infiniflow/ragflow)
- [å®˜æ–¹æ–‡æ¡£](https://ragflow.io/docs)
- [åœ¨çº¿æ¼”ç¤º](https://demo.ragflow.io)

### æŠ€æœ¯åšå®¢
- [RAGæŠ€æœ¯åŸç†è¯¦è§£](https://blog.example.com/rag-principles)
- [å‘é‡æ•°æ®åº“é€‰å‹æŒ‡å—](https://blog.example.com/vector-db-guide)
- [LLMåº”ç”¨æœ€ä½³å®è·µ](https://blog.example.com/llm-best-practices)

### å¼€æºé¡¹ç›®
- [LangChain](https://github.com/langchain-ai/langchain)
- [Chroma](https://github.com/chroma-core/chroma)
- [Ollama](https://github.com/ollama/ollama)

### å­¦æœ¯è®ºæ–‡
- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- "Dense Passage Retrieval for Open-Domain Question Answering"
- "FiD: Leveraging Passage Retrieval with Generative Models"

## ğŸ¤ å­¦ä¹ æ”¯æŒ

### ç¤¾åŒºèµ„æº
- [Discordç¤¾åŒº](https://discord.gg/NjYzJD3GM3)
- [GitHub Issues](https://github.com/infiniflow/ragflow/issues)
- æŠ€æœ¯äº¤æµç¾¤ï¼ˆå¾®ä¿¡/QQï¼‰

### è·å–å¸®åŠ©
- å®˜æ–¹æ–‡æ¡£FAQ
- ç¤¾åŒºé—®ç­”
- æŠ€æœ¯æ”¯æŒé‚®ç®±

---

*ç¥æ‚¨å­¦ä¹ æ„‰å¿«ï¼Œæ—©æ—¥æˆä¸ºRAGFlowä¸“å®¶ï¼* 